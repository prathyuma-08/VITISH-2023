{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8HXwF2Fr8Ri5nqFLF+fBo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathyuma-08/VITISH-2023/blob/main/VITISH_2023_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "ldyoTlf0P10d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import cv2\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img\n",
        "import numpy as np\n",
        "from keras.optimizers import Adam\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "import re\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText"
      ],
      "metadata": {
        "id": "4EiXai26Q3pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Extraction"
      ],
      "metadata": {
        "id": "s4PDv8t4ln_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_url = \"https://sites.google.com/site/pornographydatabase/\"\n",
        "response = requests.get(source_url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "image_urls = []\n",
        "for img in soup.find_all(\"img\"):\n",
        "    image_url = img.get(\"src\")\n",
        "    if image_url:\n",
        "        image_urls.append(image_url)\n",
        "\n",
        "print(image_urls)"
      ],
      "metadata": {
        "id": "dl0mH5JRs0ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training - Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "SCGTyYWbQf8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://figshare.com/ndownloader/files/27771210"
      ],
      "metadata": {
        "id": "qfvvgrHSAxxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/27771210"
      ],
      "metadata": {
        "id": "5A9JavlfQvll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the image data generators for training and validation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/content/image dataset/train', target_size=(299, 299), batch_size=32, class_mode='binary')\n",
        "test_generator = test_datagen.flow_from_directory('/content/image dataset/test', target_size=(299, 299), batch_size=32, class_mode='binary')"
      ],
      "metadata": {
        "id": "FM4YM2RLRwTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained InceptionV3 model\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# Add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a fully-connected layer with 1024 units and ReLU activation\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# x = Dense(512, activation='relu')(x)\n",
        "#x = Dense(256, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "# Add a final output layer with sigmoid activation for binary classification\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Define the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "fgG5TFUA7AyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5667f3b1-89cc-491b-9bf1-621473ff3585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 5s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=10, steps_per_epoch=10, validation_data=test_generator, validation_steps=10)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=10)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "tvCZgtIkUzau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model_94.h5')"
      ],
      "metadata": {
        "id": "5a0EzHPVRpQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image prediction"
      ],
      "metadata": {
        "id": "k6aJu6dxRql9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open(requests.get('https://sites.google.com/site/pornographydatabase/_/rsrc/1468849941795/home/pornDB2.png', stream=True).raw)"
      ],
      "metadata": {
        "id": "OUHbN_llhgaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess the input image\n",
        "def preprocess_image(image):\n",
        "    # Load the image\n",
        "    #image = cv2.imread(image_path)\n",
        "    # Resize the image to 224x224 pixels\n",
        "    image = np.array(image)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "    # Convert the image to a 4D tensor\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    # Subtract the mean RGB values of the ImageNet dataset\n",
        "    image = image - [123.68, 116.779, 103.939]\n",
        "    return image"
      ],
      "metadata": {
        "id": "Op3mTsQcQvFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = preprocess_image(im)\n",
        "\n",
        "# Use the ResNet50 model to make a prediction\n",
        "resnet_prediction = model.predict(img)\n",
        "\n",
        "# Print the predictions\n",
        "print('ResNet50 prediction:', resnet_prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMnQ_Ob2hwTd",
        "outputId": "bc3a7af3-9851-49b6-e7af-80efa55d8ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n",
            "ResNet50 prediction: [[5.834533e-37]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video prediction"
      ],
      "metadata": {
        "id": "yjqVQnNjSV8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set video file path of input video with name and extension\n",
        "vid = cv2.VideoCapture('/content/Trailer Launch - 5 Days to go _ PS 2 _ Mani Ratnam _ AR Rahman _ Subaskaran _ Lyca Productions.mp4')\n",
        "\n",
        "\n",
        "if not os.path.exists('images'):\n",
        "    os.makedirs('images')\n",
        "\n",
        "#for frame identity\n",
        "index = 0\n",
        "while(True):\n",
        "    # Extract images\n",
        "    ret, frame = vid.read()\n",
        "    # end of frames\n",
        "    if not ret: \n",
        "        break\n",
        "    # Saves images\n",
        "    name = './images/frame' + str(index) + '.jpg'\n",
        "    print ('Creating...' + name)\n",
        "    cv2.imwrite(name, frame)\n",
        "\n",
        "    # next frame\n",
        "    index += 1"
      ],
      "metadata": {
        "id": "67VDTSHHYWub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = r'/content/images/'\n",
        "res=[]\n",
        "\n",
        "for path in os.listdir(dir_path):\n",
        "    # check if current path is a file\n",
        "    if os.path.isfile(os.path.join(dir_path, path)):\n",
        "        res.append(dir_path+path)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "z_1mRspQSrOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in res:\n",
        "  img = preprocess_image(i)\n",
        "# Use the ResNet50 model to make a prediction\n",
        "  resnet_prediction = model.predict(img)\n",
        "# Print the predictions\n",
        "  print('Prediction:', resnet_prediction)"
      ],
      "metadata": {
        "id": "MHQPh9H0Sr-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL MODELS"
      ],
      "metadata": {
        "id": "H9Bagobo0JPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. DATA COLLECTION"
      ],
      "metadata": {
        "id": "oKm5x26Q4Pkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Data Collection\n",
        "\n",
        "# CREATE FOLDER\n",
        "def folder_create(images):\n",
        "\ttry:\n",
        "\t\tfolder_name = input(\"Enter Folder Name:- \")\n",
        "\t\t# folder creation\n",
        "\t\tos.mkdir(folder_name)\n",
        "\n",
        "\t# if folder exists with that name, ask another name\n",
        "\texcept:\n",
        "\t\tprint(\"Folder Exist with that name!\")\n",
        "\t\tfolder_create()\n",
        "\n",
        "\t# image downloading start\n",
        "\tdownload_images(images, folder_name)\n",
        "\n",
        "\n",
        "# DOWNLOAD ALL IMAGES FROM THAT URL\n",
        "def download_images(images, folder_name):\n",
        "\n",
        "\t# initial count is zero\n",
        "\tcount = 0\n",
        "\n",
        "\t# print total images found in URL\n",
        "\tprint(f\"Total {len(images)} Image Found!\")\n",
        "\n",
        "\t# checking if images is not zero\n",
        "\tif len(images) != 0:\n",
        "\t\tfor i, image in enumerate(images):\n",
        "\t\t\t# From image tag ,Fetch image Source URL\n",
        "\n",
        "\t\t\t\t\t\t# 1.data-srcset\n",
        "\t\t\t\t\t\t# 2.data-src\n",
        "\t\t\t\t\t\t# 3.data-fallback-src\n",
        "\t\t\t\t\t\t# 4.src\n",
        "\n",
        "\t\t\t# Here we will use exception handling\n",
        "\n",
        "\t\t\t# first we will search for \"data-srcset\" in img tag\n",
        "\t\t\ttry:\n",
        "\t\t\t\t# In image tag ,searching for \"data-srcset\"\n",
        "\t\t\t\timage_link = image[\"data-srcset\"]\n",
        "\t\t\t\t\n",
        "\t\t\t# then we will search for \"data-src\" in img\n",
        "\t\t\t# tag and so on..\n",
        "\t\t\texcept:\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\t# In image tag ,searching for \"data-src\"\n",
        "\t\t\t\t\timage_link = image[\"data-src\"]\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t# In image tag ,searching for \"data-fallback-src\"\n",
        "\t\t\t\t\t\timage_link = image[\"data-fallback-src\"]\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t\t# In image tag ,searching for \"src\"\n",
        "\t\t\t\t\t\t\timage_link = image[\"src\"]\n",
        "\n",
        "\t\t\t\t\t\t# if no Source URL found\n",
        "\t\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t\tpass\n",
        "\n",
        "\t\t\t# After getting Image Source URL\n",
        "\t\t\t# We will try to get the content of image\n",
        "\t\t\ttry:\n",
        "\t\t\t\tr = requests.get(image_link).content\n",
        "\t\t\t\ttry:\n",
        "\n",
        "\t\t\t\t\t# possibility of decode\n",
        "\t\t\t\t\tr = str(r, 'utf-8')\n",
        "\n",
        "\t\t\t\texcept UnicodeDecodeError:\n",
        "\n",
        "\t\t\t\t\t# After checking above condition, Image Download start\n",
        "\t\t\t\t\twith open(f\"{folder_name}/images{i+1}.jpg\", \"wb+\") as f:\n",
        "\t\t\t\t\t\tf.write(r)\n",
        "\n",
        "\t\t\t\t\t# counting number of image downloaded\n",
        "\t\t\t\t\tcount += 1\n",
        "\t\t\texcept:\n",
        "\t\t\t\tpass\n",
        "\n",
        "\t\t# There might be possible, that all\n",
        "\t\t# images not download\n",
        "\t\t# if all images download\n",
        "\t\tif count == len(images):\n",
        "\t\t\tprint(\"All Images Downloaded!\")\n",
        "\t\t\t\n",
        "\t\t# if all images not download\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"Total {count} Images Downloaded Out of {len(images)}\")\n",
        "\n",
        "# MAIN FUNCTION START\n",
        "def main(url):\n",
        "\n",
        "\t# content of URL\n",
        "\tr = requests.get(url)\n",
        "\n",
        "\t# Parse HTML Code\n",
        "\tsoup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "\t# find all images in URL\n",
        "\timages = soup.findAll('img')\n",
        "\n",
        "\t# Call folder create function\n",
        "\tfolder_create(images)\n",
        "\n",
        "\n",
        "# take url\n",
        "url = input(\"Enter URL:- \")\n",
        "\n",
        "# CALL MAIN FUNCTION\n",
        "main(url)"
      ],
      "metadata": {
        "id": "7hZFK0tFc7p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. UPSAMPLING"
      ],
      "metadata": {
        "id": "d3CXWvJ24S6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Upsampling\n",
        "\n",
        "path_cs  = 'D:\\CODING\\Python programs\\VITISH 23\\content\\celebrity-SFW'\n",
        "path_gi  = 'D:\\CODING\\Python programs\\VITISH 23\\content\\gore-images'\n",
        "path_li  = 'D:\\CODING\\Python programs\\VITISH 23\\content\\lingerie'\n",
        "path_nu  = 'D:\\CODING\\Python programs\\VITISH 23\\content\\nudity'\n",
        "path_sa  = 'D:\\CODING\\Python programs\\VITISH 23\\content\\sexual activity'\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "# Loop through images and generate augmented versions\n",
        "def upsample(image_path):\n",
        "    for filename in os.listdir(image_path):\n",
        "        img = load_img(os.path.join(image_path, filename))  # Load image\n",
        "        x = img_to_array(img)  # Convert to numpy array\n",
        "        x = x.reshape((1,) + x.shape)  # Reshape to 4D array with single sample\n",
        "        i = 0\n",
        "        for batch in datagen.flow(x, batch_size=1, save_to_dir=image_path, save_prefix=filename.split('.')[0], save_format='jpg'):\n",
        "            i += 1\n",
        "            if i > 9:  # Generate 10 augmented versions of each image\n",
        "                break\n",
        "\n",
        "upsample(path_cs)\n",
        "upsample(path_gi)\n",
        "upsample(path_li)\n",
        "upsample(path_nu)\n",
        "upsample(path_sa)"
      ],
      "metadata": {
        "id": "IYcqIDPV0X8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. WEB SCRAPING"
      ],
      "metadata": {
        "id": "aE9uS0dK4YsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Web scraping\n",
        "\n",
        "def web_scrap(web_url):\n",
        "  source_url = web_url\n",
        "  response = requests.get(source_url)\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  image_urls = []\n",
        "  for img in soup.find_all(\"img\", {\"src\": re.compile(\".*\\.(jpg|jpeg|png|gif)\")}):\n",
        "      image_url = img.get(\"src\")\n",
        "      if image_url:\n",
        "          image_urls.append(image_url)\n",
        "\n",
        "  print(image_urls)\n",
        "\n",
        "  svg_urls = []\n",
        "  for svg in soup.find_all(\"svg\"):\n",
        "      svg_url = svg.get(\"src\")\n",
        "      if svg_url:\n",
        "          svg_urls.append(svg_url)\n",
        "\n",
        "  print(svg_urls)\n",
        "\n",
        "  gif_urls = []\n",
        "  for gif in soup.find_all(\"img\", {\"src\": re.compile(\".*\\.gif\")}):\n",
        "      gif_url = gif.get(\"src\")\n",
        "      if gif_url:\n",
        "          gif_urls.append(gif_url)\n",
        "\n",
        "  print(gif_urls)\n",
        "\n",
        "  print(image_urls)\n",
        "  return image_urls"
      ],
      "metadata": {
        "id": "bh-m9VNq0X4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. IMAGE PREDICITON"
      ],
      "metadata": {
        "id": "RCvRflUA4mqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Image prediction\n",
        "\n",
        "def image_pred(web_url):\n",
        "  result_url = []\n",
        "  list_of_url = web_scrap(web_url)\n",
        "  model = keras.models.load_model('model_94.h5')\n",
        "  for f in list_of_url:\n",
        "    im = Image.open(requests.get(f, stream=True).raw)\n",
        "    img = preprocess_image(im)\n",
        "    prediction = model.predict(img)\n",
        "    print(\"Prediction: \",prediction)\n",
        "    if prediction>=0.65: #threshold\n",
        "      result_url.append(f)\n",
        "  return result_url"
      ],
      "metadata": {
        "id": "BXd5YjFu0X12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. VIDEO PREDICTION"
      ],
      "metadata": {
        "id": "sn58vnjR4u8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Video prediction\n",
        "\n",
        "def video_pred(vid_path):\n",
        "  vid = cv2.VideoCapture(vid_path)\n",
        "  if not os.path.exists('images'):\n",
        "    os.makedirs('images')\n",
        "\n",
        "#for frame identity\n",
        "  index = 0\n",
        "  while(True):\n",
        "      # Extract images\n",
        "      ret, frame = vid.read()\n",
        "      # end of frames\n",
        "      if not ret: \n",
        "          break\n",
        "      # Saves images\n",
        "      name = './images/frame' + str(index) + '.jpg'\n",
        "      print ('Creating...' + name)\n",
        "      cv2.imwrite(name, frame)\n",
        "\n",
        "      # next frame\n",
        "      index += 1\n",
        "\n",
        "  dir_path = r'/content/images/'\n",
        "  res=[]\n",
        "\n",
        "  for path in os.listdir(dir_path):\n",
        "      # check if current path is a file\n",
        "      if os.path.isfile(os.path.join(dir_path, path)):\n",
        "          res.append(dir_path+path)\n",
        "  #print(res)\n",
        "\n",
        "  for i in res:\n",
        "    img = preprocess_image(i)\n",
        "  # Use the ResNet50 model to make a prediction\n",
        "    resnet_prediction = model.predict(img)\n",
        "  # Print the predictions\n",
        "    print('Prediction:', resnet_prediction)"
      ],
      "metadata": {
        "id": "oc4H1m7a0XYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. REPORTING"
      ],
      "metadata": {
        "id": "HUUxtxHX47fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def send_mail(send_to_gmail,subject,message,url):\n",
        "    gmail= \"gmail id\"\n",
        "    password= \"password\"\n",
        "    \n",
        "    send_text = image_pred(url)\n",
        "    # storing  the information to the server\n",
        "    msg=MIMEText(send_text)\n",
        "    msg['from']=gmail\n",
        "    msg['to']=send_to_gmail\n",
        "    msg['subject']= \"Obscene Media Detected in the following URLs\"\n",
        "\n",
        "    # To connect the server\n",
        "    server= smtplib.SMTP_SSL('smtp.gmail.com',465)\n",
        "    #To login to the server\n",
        "    server.login(gmail,password)\n",
        "    # Sending the mail\n",
        "    server.sendmail(gmail,send_to_gmail,(msg.as_string()))\n",
        "    print('Mail sent')\n",
        "    # Exiting the server\n",
        "    server.quit()"
      ],
      "metadata": {
        "id": "fjJnOx51495O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}